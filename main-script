#!/bin/sh
if [ "$1" == "stop" ] ; then exit; fi 

mkdir -p /var/spool/joboutputs
chmod ugo+rwxt /var/spool/joboutputs/

(
# Cloud Init should do this automatically but something has changed since cernvm3 -> cernvm4
ls -l /root/.ssh/authorized_keys
if [ ! -s /root/.ssh/authorized_keys ] ; then
  curl http://169.254.169.254/2009-04-04/meta-data/public-keys/0/openssh-key > /root/.ssh/authorized_keys
  echo >> /root/.ssh/authorized_keys
  ls -l /root/.ssh/authorized_keys
fi

# For the big 40GB+ logical partition
mkdir -p /scratch

# Find the largest unformatted partition on any device by trying to format them
sfdisk -sldu M | grep '^/dev/.*size=' | sed 's:\(/dev/[a-z]*[0-9]*\) .*size= *\([0-9]*\).*:\2 \1:' | sort -nr | (
while read size device
do
  mkfs -q -t ext4 $device
  if [ $? = 0 ] ; then
   # If we can make a new partition, then we use it
   date --utc +"%Y-%m-%d %H:%M:%S %Z user_data_script Created $device for /scratch"
   mount $device /scratch
   break
  fi
done
)

# fallocate is almost instantaneous on ext4
fallocate -l 3G /scratch/swapfile
chmod 0600 /scratch/swapfile
mkswap /scratch/swapfile
swapon /scratch/swapfile
sysctl vm.swappiness=1
mkdir -p /scratch/condor/
chown condor:condor /scratch/condor/

cat <<EOF >/tmp/x509proxy.pem
##user_data_option_x509_proxy##
EOF
chown root.root /tmp/x509proxy.pem
chmod 0400 /tmp/x509proxy.pem
openssl x509 -in /tmp/x509proxy.pem -text > /var/spool/joboutputs/x509proxy.cert.pem

. /root/mjf.sh
export job_id=`python -c "import urllib ; print urllib.urlopen('$JOBFEATURES/job_id').read().strip()"`
export wall_limit_secs=`python -c "import urllib ; print urllib.urlopen('$JOBFEATURES/wall_limit_secs').read().strip()"`

# Send a heartbeat every 5 minutes
(
while true
do
  echo `cut -f1-3 -d' ' /proc/loadavg` `cat /proc/uptime` >/var/spool/joboutputs/heartbeat
  date --utc +"%Y-%m-%d %H:%M:%S %Z Uploading heartbeat"
  /usr/bin/curl --max-time 30 --capath /etc/grid-security/certificates/ --cert /tmp/x509proxy.pem --cacert /tmp/x509proxy.pem --location --upload-file /var/spool/joboutputs/heartbeat '##user_data_joboutputs_url##/heartbeat'
  date --utc +"%Y-%m-%d %H:%M:%S %Z curl returns $?"
  sleep 300
done
) >/var/log/heartbeat.log 2>&1 &

# Enable config-egi.egi.eu
cat <<EOF >/etc/cvmfs/default.d/60-egi.conf
CVMFS_CONFIG_REPOSITORY=config-egi.egi.eu
EOF

# Apply the CernVM-FS configuration changes
cvmfs_config reload

# Remove docker command etc since we don't start docker
rpm --nodeps -e docker-cernvm

# Hard links to Condor log files are put here, so they survive Condor attempting to delete them
mkdir -p /scratch/joblogs
chmod ugo+wxt,u+r,go-r /scratch/joblogs
( 
while :
do
sleep 120
ln -f /scratch/pilot/$job_id/glide_*/log/*Log /scratch/joblogs/
done
) &

# Set up standard WLCG WN things
ln -s /cvmfs/grid.cern.ch/etc/grid-security/vomses /etc/vomses
ln -s /cvmfs/grid.cern.ch/etc/grid-security/vomsdir /etc/grid-security/vomsdir
ln -s /cvmfs/grid.cern.ch/umd-c7wn-latest/etc/profile.d/setup-c7-wn-example.sh /etc/profile.d/
echo "export JOB_ID=$job_id QUEUE=##user_data_machinetype## GLOBUS_LOCATION=/cvmfs/grid.cern.ch/umd-c7wn-latest/usr" >/etc/profile.d/pilot.sh

# Convert the generic X.509 proxy we have into a DUNE pilot proxy
# by adding VOMS ACs to X.509 proxy 
cp /tmp/x509proxy.pem /tmp/x509vomsproxy.pem
chmod 0600 /tmp/x509vomsproxy.pem
. /etc/profile.d/setup-c7-wn-example.sh
export X509_USER_PROXY=/tmp/x509vomsproxy.pem
voms-proxy-init -noregen -valid 168:0 -voms dune:/dune/Role=pilot
voms-proxy-info -file $X509_USER_PROXY

# Set up the pilot user which runs the pilot script
useradd pilot
mkdir -p /scratch/pilot/$job_id
curl --capath /etc/grid-security/certificates https://repo.iris.ac.uk/dune/iris-dune-vm/glidein_startup.sh > /scratch/pilot/$job_id/glidein_startup.sh
chmod +x /scratch/pilot/$job_id/glidein_startup.sh

# Map Vac/Vcycle spaces to glidein entry names
case "##user_data_space##" in
*.manchester.ac.uk)
#  ENTRY=UBoone_T2_UK_Manchester_ce01
#  SIGNENTRY=13b2b07c86ee23f343e4b316a2ee3685fadc1885
  ENTRY=UBoone_T2_UK_Manchester_ce02
  SIGNENTRY=b0d578bc84318dcae81e4cb7c980627917f3594d
#  ENTRY=UBoone_T2_UK_Manchester_vac02
#  SIGNENTRY=68832e9c0d0d1dc447122944c92a04b0882b5dab
  CLIENTGROUP=dune_cern
  ;;
*)
  ENTRY=''
  CLIENTGROUP=''
  ;;
esac

# -web http://gfactory-2.opensciencegrid.org/factory/stage \
# -clientweb http://gpfrontend01.fnal.gov:8319/vofrontend/stage \
# -clientwebgroup http://gpfrontend01.fnal.gov:8319/vofrontend/stage/group_$CLIENTGROUP \

cat <<EOF > /scratch/pilot/pilot.sh
cd /scratch/pilot/$job_id
export X509_USER_PROXY=/tmp/x509vomsproxy.pem
/scratch/pilot/$job_id/glidein_startup.sh < /dev/null \
-v std \
-name ##user_data_machine_hostname##_$job_id \
-entry $ENTRY \
-clientname gpfrontend01-fnal-gov_gWMSFrontend.$CLIENTGROUP \
-schedd schedd_glideins4@gfactory-2.opensciencegrid.org \
-proxy None \
-factory OSG \
-web http://repo.iris.ac.uk/dune/glidein/web \
-sign eb88f4057055de899698e7b6c0b9c2c1dd41779f \
-signentry $SIGNENTRY \
-signtype sha1 \
-descript description.j81aAK.cfg \
-descriptentry description.j81aAK.cfg \
-dir . \
-param_GLIDEIN_Client gpfrontend01-fnal-gov_gWMSFrontend.$CLIENTGROUP \
-submitcredid 67716 \
-slotslayout fixed \
-clientweb http://repo.iris.ac.uk/dune/glidein/clientweb \
-clientsign becc2ad0f5ba634bb6c8b21eb88bace2c58072cf \
-clientsigntype sha1 \
-clientdescript description.j819gN.cfg \
-clientgroup $CLIENTGROUP \
-clientwebgroup http://repo.iris.ac.uk/dune/glidein/clientwebgroup \
-clientsigngroup f380493293d8f2c3ca531ecb59ced8ebb2659007 \
-clientdescriptgroup description.j819gN.cfg \
-param_CONDOR_VERSION 8.dot,6.dot,3 \
-param_GLIDEIN_Glexec_Use NEVER \
-param_GLIDEIN_Job_Max_Time $wall_limit_secs \
-param_SLOT_WEIGHT "ifThenElse.open,.nbsp,.open,Cpus.nbsp,.gt,.eq,.nbsp,ceiling.open,Memory/2048.dot,0.close,.close,.comma,.nbsp,Cpus.comma,.nbsp,Memory/2048.dot,0.close," \
-param_GLIDECLIENT_Rank 1 \
-param_GLIDEIN_Report_Failed NEVER \
-param_MIN_DISK_GBS 1 \
-param_GLIDEIN_Monitoring_Enabled False \
-param_CONDOR_OS default \
-param_HAS_USAGE_MODEL OFFSITE \
-param_UPDATE_COLLECTOR_WITH_TCP True \
-param_GLIDECLIENT_ReqNode gfactory.minus,2.dot,opensciencegrid.dot,org \
-param_USE_MATCH_AUTH True \
-param_CONDOR_ARCH default \
-param_FERMIHTC_DOCKER_CAPABLE False \
-param_GLIDEIN_Collector "gpcollector04.dot,fnal.dot,gov.colon,9620.minus,9625.semicolon,gpcollector03.dot,fnal.dot,gov.colon,9620.minus,9625" \
-cluster 496811 \
-subcluster 0 \
1>/var/spool/joboutputs/glidein_startup.sh.stdout \
2>/var/spool/joboutputs/glidein_startup.sh.stderr
EOF
chmod +x /scratch/pilot/pilot.sh /scratch/pilot/$job_id/glidein_startup.sh
chown -R pilot.pilot /scratch/pilot /tmp/x509vomsproxy.pem

if [ "$ENTRY" != "" ] ; then
  # Run the pilot script which in turn runs glidein_startup.sh
  /usr/bin/sudo -i -n -u pilot /scratch/pilot/pilot.sh 2>&1

  # Always try to make simple HTCondor shutdown messages
  if [ ! -s /scratch/joblogs/StartdHistoryLog ] ; then
    echo '300 No HTCondor job to run' > /var/spool/joboutputs/shutdown_message
  else
    echo '200 Success' > /var/spool/joboutputs/shutdown_message
  fi
else
  # No hardcoded mapping of space to glideinWMS entry in the case...esac above
  echo "400 No glideinWMS entry for ##user_data_space##" > /var/spool/joboutputs/shutdown_message
fi

# Time to upload and shutdown
cd /var/spool/joboutputs
cp -f /var/log/cloud-init*.log \
      /var/log/boot.log \
      /var/log/messages \
      /scratch/joblogs/* \
      .
for i in * 
do 
  curl --capath /etc/grid-security/certificates/ --cert /tmp/x509proxy.pem --cacert /tmp/x509proxy.pem --location --upload-file "$i" '##user_data_joboutputs_url##/'
  curl --capath /etc/grid-security/certificates/ --cert /tmp/x509proxy.pem --cacert /tmp/x509proxy.pem --location --upload-file "$i" "https://depo.gridpp.ac.uk/hosts/##user_data_space##/##user_data_machinetype##/##user_data_machine_hostname##/$job_id/"
done

# Try normal shutdown
shutdown -h now
sleep 60
# Otherwise instant shutdown
echo o > /proc/sysrq-trigger

) >/var/spool/joboutputs/shellscript.log 2>&1 &
